{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "THM-workshop.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Fjoelsak/highway-env/blob/THM-workshop/THM-workshop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mK_de14INQTp",
        "colab_type": "text"
      },
      "source": [
        "<p align=\"left\"> <img src=\"https://www.thm.de/_thm/logos/thm.svg\" width=\"200\"></p>\n",
        "\n",
        "# Reinforcement learning Workshop \n",
        "\n",
        "## Unsere challenge: Ein automatisiertes Parksystem\n",
        "Dazu betrachten wir das **parking-v0** Umgebungsmodell von [eleurent](https://github.com/eleurent/highway-env).\n",
        "Es liegt eine zielkonditionierte kontinuierliche Regelaufgabe vor, bei der ein Agent ein **Auto fährt**, indem er das Gaspedal and den Lenkwinkel regelt und mit einer bestimmten Ausrichtung **in einer Parklücke einparken soll**.\n",
        "Dazu betrachten wir den Use Case eines Einparkmanövers auf einer Freifläche mit eingezeichneten Linien der senkrechten Parklücken und keinen weiteren Verkehrsteilnehmern, wie sie beispielsweise auf einem Supermarktparkplatz vorliegen kann.\n",
        "\n",
        "![alt text](https://github.com/Fjoelsak/highway-env/blob/gh-media/docs/media/parking-env.gif?raw=true)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVdFv2zbu3A7",
        "colab_type": "text"
      },
      "source": [
        "Zunächst installieren wir das Umgebungsmodell und importieren einige packages zur Visualisierung."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-w1TLVvH_M-",
        "colab_type": "code",
        "outputId": "eb716330-55b3-48cc-9515-bb2095667d83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Install environment and visualization dependencies \n",
        "!pip install -q git+https://github.com/Fjoelsak/highway-env#egg=highway-env\n",
        "!pip install -q gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -q -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "\n",
        "# Environment\n",
        "import gym\n",
        "import highway_env\n",
        "import numpy as np\n",
        "\n",
        "# Visualization\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from tqdm.notebook import trange\n",
        "from IPython import display as ipythondisplay\n",
        "from pyvirtualdisplay import Display\n",
        "from gym.wrappers import Monitor\n",
        "import base64\n",
        "\n",
        "# IO\n",
        "from pathlib import Path"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pygame 1.9.6\n",
            "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OV6b1w2JNroW",
        "colab_type": "text"
      },
      "source": [
        "Zur Visualisierung bauen wir uns eine kleine Helferfunktion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3X9h1cNMisW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n",
        "\n",
        "def show_video():\n",
        "    html = []\n",
        "    for mp4 in Path(\"video\").glob(\"*.mp4\"):\n",
        "        video_b64 = base64.b64encode(mp4.read_bytes())\n",
        "        html.append('''<video alt=\"{}\" autoplay \n",
        "                      loop controls style=\"height: 300px;\">\n",
        "                      <source src=\"data:video/mp4;base64,{}\" type=\"video/mp4\" />\n",
        "                 </video>'''.format(mp4, video_b64.decode('ascii')))\n",
        "    ipythondisplay.display(ipythondisplay.HTML(data=\"<br>\".join(html)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfyhzwPDS9jG",
        "colab_type": "text"
      },
      "source": [
        "## Let's try it!\n",
        "\n",
        "Zum Ausprobieren lädt man nun das Umgebungsmodell und führt einige willkürlich ausgewählten actions aus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "829njgbSJkO4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = gym.make(\"parking-v0\")\n",
        "env = Monitor(env, './video', force=True, video_callable=lambda episode: True)\n",
        "env.reset()\n",
        "done = False\n",
        "while not done:\n",
        "    action = env.action_space.sample()\n",
        "    obs, reward, done, info = env.step(action)\n",
        "env.close()\n",
        "show_video()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXoTL5Ldvzp2",
        "colab_type": "text"
      },
      "source": [
        "Was passiert hier genau?\n",
        "\n",
        "*   die Umgebung **initialisiert** und **reseten**. \n",
        "*   in der while-Schleife wird die jeweilige **action** des Agenten festgelegt und ein Zeitschritt (**step**) durchgeführt\n",
        "\n",
        "\n",
        "das heißt, der Agent führt die Action im Umgebungsmodell aus, dass sich dementsprechend verändert. Zusätzlich wird ein reward-Signal ausgegeben. Done ist lediglich ein flag, ob das Ziel erreicht ist und info weitere Informationen zum Zeitschritt."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62Sa_CRk2Y6p",
        "colab_type": "text"
      },
      "source": [
        "Im vorgegebenen Umgebungsmodell kann ein Agent das Gaspedal (die **Beschleunigung**) und den **Lenkwinkel** regeln. Diese sind jeweils auf bestimmte Werte begrenzt. Demnach ist eine `action = [beschleunigung, lenkwinkel]`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUvkyB1K1qvF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b068ad8a-98d8-4a4d-ebba-fd60b15aa4f8"
      },
      "source": [
        "print(\"Action:\", action)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Action: [-0.8996684  0.937322 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZQtCH_N_WHB",
        "colab_type": "text"
      },
      "source": [
        "Das Umgebungsmodell ist ein `GoalEnv`, das heißt der Agent bekommt ein Dictionary, dass sowohl die derzeitige **Observation** als auch das Ziel erhält. Dabei besteht die derzeitige Observation aus \n",
        "\n",
        "*   `x, y` dem aktuellen Standort des Autos\n",
        "*   `vx, vy`der aktuellen Geschwindigkeit des Autos\n",
        "*   `cos_h, sin_h` der derzeitigen Ausrichtung des Autos\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7lxdIbz_y1W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Observation:\", obs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYZYssyfDyMc",
        "colab_type": "text"
      },
      "source": [
        "Das **reward**-Signal ist letztlich wieder skalar und wird berechnet aus einer gewichteten p-Norm der Differenz zwischen derzeitigem Status ($goal_{\\text{achieved}}$) und definiertem Ziel ($goal_{\\text{desired}}$). Dabei wurde den einzelnen Elementen der Differenz beider Stati Gewichte ($w$) zugewiesen, damit das übergeordnete Ziel: mit richtiger Ausrichtung in der Parklücke eingeparkt, erreicht wird. Die Nähe zum Zielort wird dabei besonders hoch gewichtet.\n",
        "<center>\n",
        "$ r = -|(goal_{\\text{desired}} - goal_{\\text{achieved}})\\cdot w|^{0.5}$\n",
        "</center>\n",
        "\n",
        "Dabei wird bewusst ein negativer **reward** berechnet, da das Ziel der RL Algorithmen eine Maximierung des reward Signals zum Ziel hat."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ns7W6FZeD3TF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Reward:\", reward)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDlUSTKSK-t4",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cVxmFDv9GA4",
        "colab_type": "text"
      },
      "source": [
        "## Eine mögliche Lösung\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4FXe33_9K8O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "!pip install -q stable-baselines==2.10.0\n",
        "!pip install -q git+https://github.com/Fjoelsak/highway-env#egg=highway-env\n",
        "\n",
        "# Environment\n",
        "import gym\n",
        "import highway_env\n",
        "\n",
        "# Agent\n",
        "from stable_baselines import HER, SAC"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyV2F2KT9TpH",
        "colab_type": "text"
      },
      "source": [
        "Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTJEclzv9UqH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = gym.make(\"parking-ActionRepeat-v0\")\n",
        "model = HER('MlpPolicy', env, SAC, n_sampled_goal=4,\n",
        "            goal_selection_strategy='future',\n",
        "            verbose=1, buffer_size=int(1e6),\n",
        "            learning_rate=1e-3,\n",
        "            gamma=0.9, batch_size=256,\n",
        "            policy_kwargs=dict(layers=[256, 256, 256]))\n",
        "model.learn(int(2e4))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrNUPe7C9lhD",
        "colab_type": "text"
      },
      "source": [
        "Test the policy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DuuYcANc9kvh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = gym.make(\"parking-ActionRepeat-v0\")\n",
        "env = Monitor(env, './video', force=True, video_callable=lambda episode: True)\n",
        "for episode in trange(3, desc=\"Test episodes\"):\n",
        "    obs, done = env.reset(), False\n",
        "    env.unwrapped.automatic_rendering_callback = env.video_recorder.capture_frame\n",
        "    while not done:\n",
        "        action, _ = model.predict(obs)\n",
        "        obs, reward, done, info = env.step(action)\n",
        "env.close()\n",
        "show_video()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}